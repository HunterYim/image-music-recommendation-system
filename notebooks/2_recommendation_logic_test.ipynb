{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyMiyN+0i5xj/e6OgeiXqR6Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"nePH4OmDqSvM"},"outputs":[],"source":["# --- 1. 환경 설정 ---\n","\n","# 1. 라이브러리 설치\n","!pip install -q transformers accelerate bitsandbytes\n","!pip install -q ftfy regex tqdm\n","!pip install -q git+https://github.com/openai/CLIP.git\n","!pip install -q librosa\n","\n","# 2. 라이브러리 임포트\n","import os\n","import io\n","import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from torch import nn\n","from PIL import Image\n","from google.colab import files\n","from IPython.display import Audio, display\n","from pprint import pprint\n","from transformers import (\n","    ClapModel, ClapProcessor,\n","    Blip2ForConditionalGeneration, AutoProcessor,\n","    InstructBlipForConditionalGeneration,\n","    CLIPModel, CLIPProcessor as HFCLIPProcessor\n",")\n","\n","# 3. Google Drive 마운트\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"✅ 1. 환경 설정 완료\")"]},{"cell_type":"code","source":["# --- 2. 경로 및 전역 설정 ---\n","\n","# === 기본 설정 ===\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","DTYPE_TORCH = torch.float16 if DEVICE == \"cuda\" else torch.float32\n","TOPK = 3  # 추천할 트랙 개수\n","\n","# === 파일 경로 (사용자 환경에 맞게 수정) ===\n","MUSIC_EMB_PATH = \"/content/drive/MyDrive/Jamendo_Embeddings_Final/song_embeds.npy\"\n","MUSIC_META_PATH = \"/content/drive/MyDrive/Jamendo_Embeddings_Final/song_paths.csv\"\n","CLIP_EMOTION_CKPT = \"/content/drive/MyDrive/image-music-recommendation-system_final/clip_emotion_classifier.pt\"\n","\n","# === 테스트할 이미지 경로 (사용자 환경에 맞게 수정) ===\n","IMAGE_PATHS = [\"/content/drive/MyDrive/Datasets/EmoSet-118K/image/amusement/amusement_00000.jpg\"]\n","\n","# === 모델 체크포인트 ===\n","# 음악 임베딩 만들 때 쓴 것과 동일해야 함\n","CLAP_CKPT = \"laion/clap-htsat-unfused\"\n","\n","# 짧은 캡션\n","BLIP2_CKPT = \"Salesforce/blip2-flan-t5-xl\"\n","\n","# 긴 설명\n","INSTRUCTBLIP_CKPT = \"Salesforce/instructblip-flan-t5-xl\"\n","\n","# 감정 추출 CLIP 모델\n","CLIP_CHECKPOINT = \"openai/clip-vit-base-patch32\"\n","\n","# === 감정 클래스 라벨 ===\n","EMO_LABELS = [\"amusement\", \"anger\", \"awe\", \"contentment\", \"disgust\", \"excitement\", \"fear\", \"sadness\"]\n","\n","print(f\"✅ 2. 설정 완료. Device: {DEVICE}, DType: {DTYPE_TORCH}\")"],"metadata":{"id":"xnOy_kK_smON"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 3. 모든 AI 모델 로드 ---\n","\n","# (1) CLAP\n","clap_model = ClapModel.from_pretrained(CLAP_CKPT).to(DEVICE).eval()\n","clap_processor = ClapProcessor.from_pretrained(CLAP_CKPT)\n","print(\"✅ CLAP 로드 완료.\")\n","\n","# (2) BLIP-2\n","blip2_processor = AutoProcessor.from_pretrained(BLIP2_CKPT)\n","blip2_model = Blip2ForConditionalGeneration.from_pretrained(BLIP2_CKPT, torch_dtype=DTYPE_TORCH, device_map=\"auto\").eval()\n","print(\"✅ BLIP-2 로드 완료.\")\n","\n","# (3) InstructBLIP\n","instruct_processor = AutoProcessor.from_pretrained(INSTRUCTBLIP_CKPT)\n","instruct_model = InstructBlipForConditionalGeneration.from_pretrained(INSTRUCTBLIP_CKPT, torch_dtype=DTYPE_TORCH, device_map=\"auto\").eval()\n","print(\"✅ InstructBLIP 로드 완료.\")\n","\n","# (4) Fine-tuned CLIP Emotion Classifier\n","class CLIPEmotionHead(nn.Module):\n","    def __init__(self, in_dim, num_classes):\n","        super().__init__()\n","        self.classifier = nn.Linear(in_dim, num_classes)\n","    def forward(self, x): return self.classifier(x)\n","\n","# --- CLIP 백본 로드 ---\n","clip_backbone = CLIPModel.from_pretrained(CLIP_CHECKPOINT, torch_dtype=DTYPE_TORCH).to(DEVICE).eval()\n","clip_proc = HFCLIPProcessor.from_pretrained(CLIP_CHECKPOINT)\n","\n","# --- Emotion Head 인스턴스 생성 및 가중치 로드 ---\n","IN_DIM = clip_backbone.config.projection_dim\n","emotion_head = CLIPEmotionHead(in_dim=IN_DIM, num_classes=len(EMO_LABELS)).to(DEVICE, dtype=DTYPE_TORCH).eval()\n","\n","# 체크포인트(.pth)에서 가중치를 그대로 로드\n","ckpt = torch.load(CLIP_EMOTION_CKPT, map_location=DEVICE)\n","\n","if \"classifier.weight\" in ckpt:\n","    # 모델 전체 또는 헤드 전체가 저장된 경우\n","    final_state_dict = {k: v for k, v in ckpt.items() if k.startswith(\"classifier.\")}\n","    emotion_head.load_state_dict(final_state_dict)\n","else:\n","    # 분류기 부분만 저장된 경우 (키에 'classifier.' 접두사가 없음)\n","    final_state_dict = {k: v for k, v in ckpt.items() if k.startswith(\"classifier.\")}\n","    if not final_state_dict:\n","        try:\n","             emotion_head.classifier.load_state_dict(ckpt)\n","        except RuntimeError:\n","             final_state_dict = {k.replace(\"classifier.\", \"\"): v for k, v in ckpt.items() if \"classifier.\" in k}\n","             emotion_head.classifier.load_state_dict(final_state_dict)\n","\n","    else:\n","        emotion_head.load_state_dict(final_state_dict)\n","print(\"✅ CLIP 로드 완료.\")\n","\n","print(\"✅ 3. 모든 모델 로드 완료\")"],"metadata":{"id":"2vtmmGvYvldM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4. 음악 DB 로드 및 핵심 로직 함수 정의 ---\n","\n","# (1) 음악 DB 로드\n","song_embeds = torch.from_numpy(np.load(MUSIC_EMB_PATH)).float().to(DEVICE)\n","song_meta = pd.read_csv(MUSIC_META_PATH)\n","song_embeds = song_embeds / song_embeds.norm(dim=-1, keepdim=True)\n","print(f\"✅ 음악 DB 로드 완료: {len(song_meta)} 곡\")\n","\n","# (2) AI 파이프라인 함수들\n","@torch.inference_mode()\n","def predict_emotion(image):\n","    # processor 출력도 모델 dtype으로 캐스팅\n","    inputs = clip_proc(images=image, return_tensors=\"pt\").to(DEVICE, dtype=DTYPE_TORCH)\n","\n","    # CLIP 비주얼 임베딩 (dtype 일치)\n","    img_feats = clip_backbone.get_image_features(**inputs)    # [1, D], dtype=DTYPE_TORCH\n","\n","    # 로짓 → 온도 스케일 → 소프트맥스\n","    logits = emotion_head(img_feats)                          # [1, C], dtype=DTYPE_TORCH\n","    probs = torch.softmax(logits.float(), dim=-1)[0]          # 안정성 위해 softmax만 float32\n","\n","    top_id = torch.argmax(probs).item()\n","    return {\n","        \"label\": EMO_LABELS[top_id],\n","        \"prob\": probs[top_id].item(),\n","        \"all_probs\": probs.cpu().numpy()\n","    }\n","\n","@torch.inference_mode()\n","def blip2_caption(image):\n","    inputs = blip2_processor(images=image, return_tensors=\"pt\").to(DEVICE, DTYPE_TORCH)\n","    out = blip2_model.generate(**inputs, max_new_tokens=30)\n","    return blip2_processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n","\n","@torch.inference_mode()\n","def instructblip_describe(image):\n","    prompt = \"Write a detailed, vivid description of the image focusing on mood, scene, colors, lighting, and context.\"\n","    inputs = instruct_processor(images=image, text=prompt, return_tensors=\"pt\").to(DEVICE, DTYPE_TORCH)\n","    out = instruct_model.generate(**inputs, max_new_tokens=160)\n","    return instruct_processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n","\n","def build_texts_for_image(pil_img):\n","    cap_short = blip2_caption(pil_img)\n","    emo = predict_emotion(pil_img)\n","    cap_combo = f\"This image conveys {emo['label']} and feels like {emo['label']} mood. Caption: {cap_short}\"\n","    cap_long = instructblip_describe(pil_img)\n","    return cap_short, cap_combo, cap_long, emo\n","\n","@torch.inference_mode()\n","def score_all_songs_with_text(text: str):\n","    inputs = clap_processor(text=[text], return_tensors=\"pt\", padding=True).to(DEVICE)\n","    text_emb = clap_model.get_text_features(**inputs)\n","    text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n","    sims = text_emb @ song_embeds.T\n","    return sims.squeeze(0)\n","\n","def topk_emotions_from_probs(prob_array, labels, k=3):\n","    idx = np.asarray(prob_array).argsort()[::-1][:k]\n","    return [(labels[i], float(prob_array[i])) for i in idx]\n","\n","def show_topk_audio(sims_1d, k=3, title=\"Results\"):\n","    sims_cpu = sims_1d.detach().float().cpu().numpy()\n","    idx = sims_cpu.argsort()[::-1][:k]\n","\n","    print(f\"\\n== {title} (Top-{k}) ==\")\n","\n","    for r, i in enumerate(idx, start=1):\n","        score = float(sims_cpu[i])\n","        row = song_meta.iloc[i]\n","\n","        full_path = row.get(\"path\", \"N/A\")\n","\n","        try:\n","            dir_path, file_name = os.path.split(full_path)\n","            parent_dir_path, dir_name = os.path.split(dir_path)\n","            meaningful_path = f\"{dir_name}/{file_name}\"\n","        except:\n","            meaningful_path = full_path # 경로 파싱 실패 시 원본 경로 표시\n","\n","        print(f\"[{r:02d}] score={score:.3f} | title='{meaningful_path}'\")\n","\n","        try:\n","            display(Audio(filename=full_path))\n","        except Exception as e:\n","            print(f\"  (Audio preview failed for path: {full_path}. Error: {e})\")\n","\n","# (3) 메인 데모 함수\n","def demo_recommend_3plus3(pil_image, lam=0.6, top_k=3):\n","    # 텍스트 쿼리 3종 생성\n","    cap_short, cap_combo, cap_long, emo = build_texts_for_image(pil_image)\n","\n","    # 결과 헤더 출력\n","    print(\"=\"*100)\n","    print(f\"[IMAGE ANALYSIS RESULTS]\")\n","    # top-3 감정 포매팅\n","    emo_top3 = topk_emotions_from_probs(emo[\"all_probs\"], EMO_LABELS, k=3)\n","    print(\"CLIP Emotions (Top-3):\", \", \".join([f\"{e}({p:.2f})\" for e,p in emo_top3]))\n","    print(f\"BLIP-2 Caption: {cap_short}\")\n","    print(f\"InstructBLIP Caption (Long): {cap_long}\")\n","    print(\"=\"*100)\n","\n","    # Stage-1 추천 (BLIP2+CLIP 융합 쿼리)\n","    s1 = score_all_songs_with_text(cap_combo)\n","    show_topk_audio(s1, k=top_k, title=\"▶ Stage-1 Recommendations (Emotion + Facts)\")\n","\n","    # Stage-2 재랭킹 (InstructBLIP 상세 묘사 추가)\n","    s2 = score_all_songs_with_text(cap_long)\n","    fused_scores = lam * s1 + (1.0 - lam) * s2\n","    show_topk_audio(fused_scores, k=top_k, title=f\"▶ Stage-2 Re-ranked Recommendations (Context Enhanced)\")\n","\n","print(\"✅ 4. 핵심 로직 함수 정의 완료\")"],"metadata":{"id":"S4CwQfHTx2Vr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 5. 데모 실행 ---\n","\n","# 사용자 인터페이스 및 실행\n","print(\"--- 데모 실행 중... ---\")\n","print(\"이미지 파일을 업로드 하세요:\")\n","\n","# 파일 업로드 API 호출\n","uploaded = files.upload()\n","\n","# 파일이 성공적으로 업로드되었는지 확인\n","if uploaded:\n","    # 업로드된 파일 정보 가져오기\n","    filename = next(iter(uploaded))\n","    image_bytes = uploaded[filename]\n","\n","    print(f\"\\n'{filename}' 업로드 성공.\")\n","\n","    # Bytes 데이터를 PIL 이미지 객체로 변환\n","    try:\n","        pil_image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n","\n","        # 업로드된 이미지 미리보기\n","        plt.figure(figsize=(6, 6))\n","        plt.imshow(pil_image)\n","        plt.title(\"uploaded image\")\n","        plt.axis('off')\n","        plt.show()\n","\n","        # 메인 파이프라인 실행\n","        demo_recommend_3plus3(pil_image, lam=0.6, top_k=3)\n","\n","    except Exception as e:\n","        print(f\"❌ 처리 중 오류 발생: {e}\")\n","\n","else:\n","    print(\"업로드된 이미지가 없습니다.\")"],"metadata":{"id":"jNilU96KyC5i"},"execution_count":null,"outputs":[]}]}