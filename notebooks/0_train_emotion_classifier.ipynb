{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"A100","authorship_tag":"ABX9TyP4eSrXX70hUkF+4UwhIUxc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aMOmhREtMIoX"},"outputs":[],"source":["# --- 1. í™˜ê²½ ì„¤ì • ---\n","\n","# 1. í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜\n","print(\"Installing necessary libraries...\")\n","!pip install -q transformers ftfy regex tqdm scikit-learn\n","\n","# 2. ëª¨ë“  ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import os\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from transformers import CLIPModel, CLIPProcessor\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import numpy as np\n","\n","# 3. Google Drive ë§ˆìš´íŠ¸\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","print(\"\\nâœ… 1. í™˜ê²½ ì„¤ì • ì™„ë£Œ\")"]},{"cell_type":"code","source":["# --- 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ---\n","\n","# 1. ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • ë° ì••ì¶• í•´ì œ\n","DATASET_ZIP_PATH = \"/content/drive/MyDrive/Datasets/EmoSet-118K.zip\"\n","LOCAL_DATA_PATH = \"/content/emoset\"\n","\n","if not os.path.exists(LOCAL_DATA_PATH):\n","    print(f\"Unzipping dataset from {DATASET_ZIP_PATH} to {LOCAL_DATA_PATH}...\")\n","    !unzip -q \"{DATASET_ZIP_PATH}\" -d \"{LOCAL_DATA_PATH}\"\n","    print(\"Unzipping complete.\")\n","else:\n","    print(f\"Dataset already exists at {LOCAL_DATA_PATH}.\")\n","\n","\n","# 2. ì´ë¯¸ì§€ ì „ì²˜ë¦¬ê¸°(Transform) ì •ì˜\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4815, 0.4578, 0.4082), (0.2686, 0.2613, 0.2758))\n","])\n","print(\"\\nImage transform is ready.\")\n","\n","\n","# 3. PyTorch Dataset í´ë˜ìŠ¤ ì •ì˜\n","class EmoSetDataset(Dataset):\n","    def __init__(self, json_path, base_path, transform=None):\n","        with open(json_path, \"r\") as f:\n","            raw_data = json.load(f)\n","\n","        self.samples = [{'image_path': os.path.join(base_path, s[1]), 'label': s[0]} for s in raw_data]\n","        self.transform = transform\n","\n","        self.labels = sorted(list({s['label'] for s in self.samples}))\n","        self.label2id = {label: i for i, label in enumerate(self.labels)}\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","        img_path = sample['image_path']\n","        label_id = self.label2id[sample['label']]\n","\n","        try:\n","            image = Image.open(img_path).convert(\"RGB\")\n","        except FileNotFoundError:\n","            print(f\"Warning: Image not found at {img_path}. Returning a black image.\")\n","            image = Image.new(\"RGB\", (224, 224)) # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ëŒ€ë¹„\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, label_id\n","\n","print(\"PyTorch Dataset class 'EmoSetDataset' is defined.\")\n","\n","\n","# 4. ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n","train_dataset = EmoSetDataset(json_path=os.path.join(LOCAL_DATA_PATH, \"train.json\"), base_path=LOCAL_DATA_PATH, transform=transform)\n","val_dataset = EmoSetDataset(json_path=os.path.join(LOCAL_DATA_PATH, \"val.json\"), base_path=LOCAL_DATA_PATH, transform=transform)\n","\n","# DataLoaderëŠ” Datasetì„ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¶ì–´ì£¼ê³ , ë°ì´í„°ë¥¼ ì„ì–´ì£¼ëŠ” ì—­í• ì„ í•¨\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n","val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, pin_memory=True)\n","\n","# ì´í›„ ëª¨ë¸ì—ì„œ ì‚¬ìš©í•  í´ë˜ìŠ¤ ì •ë³´ ì €ì¥\n","NUM_CLASSES = len(train_dataset.labels)\n","CLASS_NAMES = train_dataset.labels\n","\n","print(f\"\\nDataset and DataLoader are ready.\")\n","print(f\" - Found {NUM_CLASSES} classes: {CLASS_NAMES}\")\n","print(f\" - Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n","\n","print(\"\\nâœ… 2. ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ\")"],"metadata":{"id":"hXteHh_1WYtk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 3. ëª¨ë¸ ì•„í‚¤í…ì²˜ ì •ì˜ ---\n","\n","# 1. ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ì •ì˜\n","class CLIPForEmotionClassification(nn.Module):\n","    def __init__(self, num_classes):\n","        super().__init__()\n","\n","        # 1-1. Hugging Face Hubì—ì„œ ì‚¬ì „ í•™ìŠµëœ CLIP ëª¨ë¸(\"openai/clip-vit-base-patch32\")ë¡œë“œ\n","        self.clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n","\n","        # 1-2. CLIP ë°±ë³¸ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ë™ê²°(freeze)\n","        for param in self.clip.parameters():\n","            param.requires_grad = False\n","\n","        # 1-3. ë¶„ë¥˜ê¸° í—¤ë“œì˜ ì…ë ¥ ì°¨ì›ì„ CLIP ëª¨ë¸ì˜ ì„¤ì •ì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´\n","        embedding_dim = self.clip.config.projection_dim\n","\n","        # 1-4. ìƒˆë¡œìš´ ë¶„ë¥˜ê¸° í—¤ë“œ ì •ì˜\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, pixel_values):\n","        # 2-1. ë™ê²°ëœ CLIP ëª¨ë¸ë¡œ ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ\n","        with torch.no_grad():\n","            image_embeds = self.clip.get_image_features(pixel_values=pixel_values)\n","\n","        # 2-2. ì¶”ì¶œëœ íŠ¹ì§•ì„ í•™ìŠµ ê°€ëŠ¥í•œ ë¶„ë¥˜ê¸° í—¤ë“œì— í†µê³¼ì‹œì¼œ ìµœì¢… ì ìˆ˜(logits) íšë“\n","        logits = self.classifier(image_embeds)\n","        return logits\n","\n","print(\"Custom model class 'CLIPForEmotionClassification' is defined.\")\n","print(\"\\nâœ… 3. ëª¨ë¸ ì •ì˜ ì™„ë£Œ\")"],"metadata":{"id":"QsJiWSHCYAXk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 4. í›ˆë ¨ ì¤€ë¹„ ---\n","\n","# 1. ì¥ì¹˜(Device) ì„¤ì •\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# 2. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\n","model = CLIPForEmotionClassification(num_classes=NUM_CLASSES).to(device)\n","\n","\n","# 3. ì†ì‹¤ í•¨ìˆ˜(Loss Function) ì •ì˜\n","# ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ, í‘œì¤€ì ì¸ CrossEntropyLoss ì‚¬ìš©\n","# ì´ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡(logits)ê³¼ ì‹¤ì œ ì •ë‹µ(label) ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ì—­í• \n","criterion = nn.CrossEntropyLoss()\n","\n","\n","# 4. ì˜µí‹°ë§ˆì´ì €(Optimizer) ì •ì˜\n","# model.parameters()ê°€ ì•„ë‹Œ, model.classifier.parameters()ë¥¼ ì „ë‹¬í•˜ì—¬ ë¶„ë¥˜ê¸° í—¤ë“œì˜ ê°€ì¤‘ì¹˜ë§Œ í•™ìŠµ(ì—…ë°ì´íŠ¸)\n","optimizer = torch.optim.Adam(model.classifier.parameters(), lr=1e-4) # í•™ìŠµë¥ (learning rate) = 1e-4\n","\n","\n","# 5. í‰ê°€ í•¨ìˆ˜(Evaluation Function) ì •ì˜\n","@torch.inference_mode()\n","def evaluate(model, dataloader):\n","    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“±ì„ ë¹„í™œì„±í™”)\n","\n","    correct_predictions = 0\n","    total_samples = 0\n","\n","    # dataloaderë¡œë¶€í„° ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë°ì´í„° ë¡œë“œ\n","    for images, labels in dataloader:\n","        # ë°ì´í„°ë¥¼ ëª¨ë¸ê³¼ ë™ì¼í•œ ì¥ì¹˜ë¡œ ì´ë™\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # ìˆœì „íŒŒë¥¼ í†µí•´ ì˜ˆì¸¡ ìˆ˜í–‰\n","        outputs = model(images)\n","\n","        # ê°€ì¥ ë†’ì€ ì ìˆ˜(logit)ë¥¼ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡ ê²°ê³¼ë¡œ ì„ íƒ\n","        predicted_labels = outputs.argmax(dim=1)\n","\n","        # ì˜ˆì¸¡ì´ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ” ê°œìˆ˜ ëˆ„ì \n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        total_samples += labels.size(0)\n","\n","    # ì „ì²´ ì •í™•ë„ ê³„ì‚° í›„ ë°˜í™˜\n","    accuracy = correct_predictions / total_samples\n","    return accuracy\n","\n","print(\"\\nModel, Criterion, Optimizer, and Evaluation function are ready.\")\n","print(\"\\nâœ… 4. í›ˆë ¨ ì¤€ë¹„ ì™„ë£Œ\")"],"metadata":{"id":"c2mmIrq6YyAI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 5. ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰ ---\n","\n","# 1. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •\n","EPOCHS = 3\n","best_val_acc = 0.0\n","best_model_path = \"/content/drive/MyDrive/image-music-recommendation-system_final/clip_emotion_classifier.pt\" # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ì„ ì €ì¥í•  ê²½ë¡œ\n","\n","# í›ˆë ¨ ê³¼ì • ê¸°ë¡ì„ ìœ„í•œ ë”•ì…”ë„ˆë¦¬\n","history = {'train_loss': [], 'train_acc': [], 'val_acc': []}\n","\n","print(f\"--- Starting training for {EPOCHS} epochs ---\")\n","\n","# 2. í›ˆë ¨ ë° ê²€ì¦ ë£¨í”„ ì‹¤í–‰\n","for epoch in range(EPOCHS):\n","    # --- í›ˆë ¨ ë‹¨ê³„ (Training Phase) ---\n","    model.train() # ëª¨ë¸ì„ í›ˆë ¨ ëª¨ë“œë¡œ ì„¤ì • (Dropout ë“± í™œì„±í™”)\n","\n","    running_loss = 0.0\n","    correct_train = 0\n","    total_train = 0\n","\n","    # tqdmì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ì§„í–‰ë¥  ì‹œê°ì  í‘œì‹œ\n","    train_progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{EPOCHS} [Training]\")\n","\n","    for images, labels in train_progress_bar:\n","        # ë°ì´í„°ë¥¼ GPUë¡œ ì´ë™\n","        images, labels = images.to(device), labels.to(device)\n","\n","        # ì˜µí‹°ë§ˆì´ì €ì˜ ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n","        optimizer.zero_grad()\n","\n","        # ìˆœì „íŒŒ: ì´ë¯¸ì§€ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡(logits) ìƒì„±\n","        outputs = model(images)\n","\n","        # ì†ì‹¤ ê³„ì‚°\n","        loss = criterion(outputs, labels)\n","\n","        # ì—­ì „íŒŒ: ì†ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ìš¸ê¸° ê³„ì‚°\n","        loss.backward()\n","\n","        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸: ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜µí‹°ë§ˆì´ì €ê°€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ì •\n","        optimizer.step()\n","\n","        # í†µê³„ ê¸°ë¡\n","        running_loss += loss.item()\n","        preds = outputs.argmax(dim=1)\n","        correct_train += (preds == labels).sum().item()\n","        total_train += labels.size(0)\n","\n","        # ì§„í–‰ë¥  í‘œì‹œì¤„ì— í˜„ì¬ ë°°ì¹˜ ì†ì‹¤ í‘œì‹œ\n","        train_progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n","\n","    # --- ê²€ì¦ ë‹¨ê³„ (Validation Phase) ---\n","    # Cell 4ì—ì„œ ì •ì˜í•œ evaluate í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì •í™•ë„ ê³„ì‚°\n","    val_acc = evaluate(model, val_loader)\n","\n","    # í•œ ì—í¬í¬ì˜ í‰ê·  ì†ì‹¤ê³¼ í›ˆë ¨ ì •í™•ë„ ê³„ì‚°\n","    train_loss = running_loss / len(train_loader)\n","    train_acc = correct_train / total_train\n","\n","    # í›ˆë ¨ ê³¼ì • ê¸°ë¡\n","    history['train_loss'].append(train_loss)\n","    history['train_acc'].append(train_acc)\n","    history['val_acc'].append(val_acc)\n","\n","    # --- ê²°ê³¼ ì¶œë ¥ ---\n","    print(f\"Epoch {epoch+1}/{EPOCHS} | \"\n","          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n","          f\"Val Acc: {val_acc:.4f}\")\n","\n","    # --- ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì €ì¥ ---\n","    # í˜„ì¬ ê²€ì¦ ì •í™•ë„ê°€ ê¸°ë¡ëœ ìµœê³  ì •í™•ë„ë³´ë‹¤ ë†’ìœ¼ë©´, ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•¨\n","    if val_acc > best_val_acc:\n","        best_val_acc = val_acc\n","        torch.save(model.state_dict(), best_model_path)\n","        print(f\"ğŸ‰ New best model saved with Val Acc: {best_val_acc:.4f} to {best_model_path}\")\n","\n","print(f\"\\n--- Training finished ---\")\n","print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n","\n","print(\"\\nâœ… 5. ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ\")"],"metadata":{"id":"YksAqix7cAQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 6. í›ˆë ¨ ê²°ê³¼ ì‹œê°í™” ë° ì„±ëŠ¥ ë¶„ì„ ---\n","\n","# 1. í›ˆë ¨ ê³¼ì • ê·¸ë˜í”„ ì‹œê°í™” (í•™ìŠµ ê³¡ì„ )\n","# í›ˆë ¨ ì†ì‹¤(Training Loss)ê³¼ ê²€ì¦ ì •í™•ë„(Validation Accuracy)ì˜ ë³€í™”ë¥¼ ì—í¬í¬ë³„ë¡œ ì‹œê°í™”\n","print(\"--- Plotting Training and Validation Metrics ---\")\n","\n","if 'history' in locals():\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n","\n","    epochs = range(1, len(history['train_loss']) + 1)\n","\n","    # í›ˆë ¨ ì†ì‹¤ ê·¸ë˜í”„\n","    ax1.plot(epochs, history['train_loss'], marker='o', linestyle='-', color='b')\n","    ax1.set_title('Training Loss Over Epochs')\n","    ax1.set_xlabel('Epoch')\n","    ax1.set_ylabel('Loss')\n","    ax1.grid(True)\n","\n","    # ê²€ì¦ ì •í™•ë„ ê·¸ë˜í”„\n","    ax2.plot(epochs, history['val_acc'], marker='o', linestyle='-', color='r')\n","    ax2.set_title('Validation Accuracy Over Epochs')\n","    ax2.set_xlabel('Epoch')\n","    ax2.set_ylabel('Accuracy')\n","    ax2.grid(True)\n","\n","    plt.suptitle('Training and Validation Metrics', fontsize=16)\n","    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","    plt.show()\n","else:\n","    print(\"'history' dictionary not found. Please run the training cell (Cell 5) first.\")\n","\n","\n","# 2. í˜¼ë™ í–‰ë ¬ (Confusion Matrix) ì‹œê°í™”\n","print(\"\\n--- Generating Confusion Matrix for the fine-tuned model ---\")\n","\n","# 2-1. ì˜ˆì¸¡ ë° ì‹¤ì œ ë ˆì´ë¸” ìˆ˜ì§‘ì„ ìœ„í•œ í•¨ìˆ˜ ì •ì˜\n","@torch.inference_mode()\n","def get_all_preds_and_labels(model, dataloader):\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","\n","    for images, labels in tqdm(dataloader, desc=\"Getting predictions for Confusion Matrix\"):\n","        images, labels = images.to(device), labels.to(device)\n","        outputs = model(images)\n","        preds = outputs.argmax(dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    return np.array(all_labels), np.array(all_preds)\n","\n","# 2-2. ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰\n","# ìµœê³  ì„±ëŠ¥ì„ ë³´ì¸ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ í‰ê°€\n","print(\"Loading the best model for evaluation...\")\n","\n","# Cell 4ì—ì„œ ì •ì˜í•œ ëª¨ë¸ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n","best_model = CLIPForEmotionClassification(num_classes=NUM_CLASSES).to(device)\n","best_model.load_state_dict(torch.load(best_model_path)) # Cell 5ì—ì„œ ì €ì¥í•œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ\n","\n","true_labels, pred_labels = get_all_preds_and_labels(best_model, val_loader)\n","\n","# 2-3. í˜¼ë™ í–‰ë ¬ ê³„ì‚° ë° ì‹œê°í™”\n","cm = confusion_matrix(true_labels, pred_labels)\n","# ê° í´ë˜ìŠ¤ë³„ ì •ë‹µ ê°œìˆ˜ë¡œ ë‚˜ëˆ„ì–´ ì •ê·œí™” (ë¹„ìœ¨ë¡œ í‘œì‹œ)\n","cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","plt.figure(figsize=(12, 10))\n","sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n","plt.title('Confusion Matrix (Normalized)', fontsize=16)\n","plt.ylabel('True Label', fontsize=12)\n","plt.xlabel('Predicted Label', fontsize=12)\n","plt.xticks(rotation=45, ha='right')\n","plt.yticks(rotation=0)\n","plt.tight_layout()\n","plt.show()\n","\n","print(\"\\nâœ… 6. ì„±ëŠ¥ ì‹œê°í™” ë° ë¶„ì„ ì™„ë£Œ\")"],"metadata":{"id":"owz3gClLdMtc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- 7. Zero-Shot CLIP ëª¨ë¸ê³¼ ì„±ëŠ¥ ë¹„êµ ---\n","print(\"--- Evaluating the original Zero-Shot CLIP model for comparison ---\")\n","\n","# 1. Zero-Shot ì˜ˆì¸¡ì„ ìœ„í•œ ì›ë³¸ CLIP ëª¨ë¸ ë¡œë“œ\n","zeroshot_clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n","zeroshot_clip_model.eval()\n","\n","# 2. í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„\n","# ì œë¡œìƒ· ë¶„ë¥˜ë¥¼ ìœ„í•´, ê° ê°ì • í´ë˜ìŠ¤ë¥¼ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±\n","text_prompts = [f\"a photo that expresses {label}\" for label in CLASS_NAMES]\n","text_inputs = processor(\n","    text=text_prompts,\n","    return_tensors=\"pt\",\n","    padding=True\n",").to(device)\n","\n","# 3. ì œë¡œìƒ· ì˜ˆì¸¡ ìˆ˜í–‰ í•¨ìˆ˜ ì •ì˜\n","@torch.inference_mode()\n","def zeroshot_predict_labels(dataloader):\n","    all_preds = []\n","    all_labels = []\n","\n","    text_features = zeroshot_clip_model.get_text_features(**text_inputs)\n","    text_features /= text_features.norm(dim=-1, keepdim=True)\n","\n","    for images, labels in tqdm(dataloader, desc=\"Running Zero-Shot predictions\"):\n","        images = images.to(device)\n","\n","        # ì´ë¯¸ì§€ íŠ¹ì§• ì¶”ì¶œ ë° ì •ê·œí™”\n","        image_features = zeroshot_clip_model.get_image_features(pixel_values=images)\n","        image_features /= image_features.norm(dim=-1, keepdim=True)\n","\n","        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°\n","        logits = image_features @ text_features.T\n","        preds = logits.argmax(dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    return np.array(all_labels), np.array(all_preds)\n","\n","# 4. ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•´ ì œë¡œìƒ· ì˜ˆì¸¡ ì‹¤í–‰\n","true_zs, pred_zs = zeroshot_predict_labels(val_loader)\n","\n","# 5. í˜¼ë™ í–‰ë ¬ ë¹„êµ ì‹œê°í™”\n","print(\"\\n--- Comparing Confusion Matrices: Fine-tuned vs. Zero-shot ---\")\n","if 'cm_normalized' not in locals():\n","    print(\"Running fine-tuned model prediction again for comparison...\")\n","    true_labels, pred_labels = get_all_preds_and_labels(best_model, val_loader)\n","    cm = confusion_matrix(true_labels, pred_labels)\n","    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","cm_zs = confusion_matrix(true_zs, pred_zs)\n","cm_zs_norm = cm_zs.astype('float') / cm_zs.sum(axis=1)[:, np.newaxis]\n","\n","fig, axes = plt.subplots(1, 2, figsize=(24, 10))\n","fig.suptitle('Confusion Matrix Comparison', fontsize=20)\n","\n","# Fine-tuned ëª¨ë¸ í˜¼ë™ í–‰ë ¬\n","sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[0])\n","axes[0].set_title('Fine-tuned Model', fontsize=16)\n","axes[0].set_xlabel('Predicted Label')\n","axes[0].set_ylabel('True Label')\n","plt.setp(axes[0].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","\n","\n","# Zero-shot ëª¨ë¸ í˜¼ë™ í–‰ë ¬\n","sns.heatmap(cm_zs_norm, annot=True, fmt='.2f', cmap='Reds',\n","            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES, ax=axes[1])\n","axes[1].set_title('Zero-shot CLIP (Original)', fontsize=16)\n","axes[1].set_xlabel('Predicted Label')\n","axes[1].set_ylabel('True Label')\n","plt.setp(axes[1].get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n","\n","\n","plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n","plt.show()\n","\n","# 6. ìµœì¢… ì •í™•ë„ ë¹„êµ ì¶œë ¥\n","from sklearn.metrics import accuracy_score\n","print(\"\\n--- Final Accuracy Comparison ---\")\n","if 'true_labels' in locals() and 'pred_labels' in locals():\n","    print(f\"Fine-tuned Model Accuracy: {accuracy_score(true_labels, pred_labels) * 100:.2f}%\")\n","else:\n","    print(\"Fine-tuned results not found. Please run Cell 6 first.\")\n","print(f\"Zero-shot Model Accuracy:  {accuracy_score(true_zs, pred_zs) * 100:.2f}%\")\n","\n","print(\"\\nâœ… 7. ì„±ëŠ¥ ë¹„êµ ë¶„ì„ ì™„ë£Œ\")"],"metadata":{"id":"iHbHJb9KfIao"},"execution_count":null,"outputs":[]}]}